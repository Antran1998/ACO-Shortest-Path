\section{Backtracking}

\subsection{Idea}
Standard ACO implementations suffer from the deadlock problem: when an ant encounters a node where all neighboring cells are either obstacles or already visited, the ant becomes trapped and must abandon its current search attempt. This premature termination wastes computational resources and reduces the algorithm's exploration efficiency, especially in maps with narrow corridors, dead-ends or complex obstacle configurations.

To address this limitation, an intelligent backtracking mechanism is introduced that allows ants to perform controlled, multi-step backtracking when encountering dead-ends. Instead of immediately restarting from scratch, ants can revisit recent decision points nodes where multiple path choices were available and explore previously unexplored alternatives. This approach transforms deadlocks from terminal failures into temporary setbacks, significantly improving path discovery rates in constrained environments.

\subsection{Implementation}
The backtracking mechanism is implemented through three complementary data structures and algorithms integrated into the \texttt{Ant} class:

\textbf{Decision Stack:} Each ant maintains a stack of decision points recorded during path traversal. A decision point is saved whenever the ant encounters a node with multiple unvisited neighbors (line 46-53 in the code). Each stack entry stores:
\begin{itemize}
    \item The node position where the decision was made
    \item The path length at that point (for restoration)
    \item The set of unexplored neighbor nodes
\end{itemize}

\textbf{Multi-Step Backtracking Algorithm:} When an ant becomes trapped (all neighbors visited or blocked), the \texttt{backtrack\_to\_decision\_point} method (lines 55-77) is invoked. The algorithm iterates through the decision stack in reverse chronological order, searching for a decision point that still has unexplored alternatives. When such a point is found:
\begin{enumerate}
    \item The ant's path is truncated to the decision point
    \item Nodes visited after that point are removed from the visited set
    \item The ant resumes exploration from the decision point with fresh options
\end{enumerate}

If no valid backtrack point exists (all decision points exhausted), the ant has encountered a true structural deadlock and terminates.

\textbf{Global Tabu List:} To prevent repeated failures at problematic nodes, a colony-level tabu list tracks nodes that have caused persistent deadlocks. When an ant exhausts all backtracking attempts at a node, that node is added to the global tabu list and avoided by all subsequent ants in the current iteration (lines 179, 204).

\textbf{Adaptive Path Quality Penalty:} During next-node selection, the algorithm applies an adaptive penalty to discourage paths that repeatedly backtrack or show poor progress toward the goal. The penalty factor ranges from 0.05 to 0.2 based on the current path quality metric, which combines distance to goal with path straightness (lines 250-257).

\textbf{Backtracking Limits:} Each ant is restricted to a maximum of 10 backtracking operations per search attempt (\texttt{max\_backtracks} parameter) to prevent infinite loops and ensure computational efficiency.

\subsection{Benefits}
The backtracking mechanism provides several key advantages:

\textbf{Increased Success Rate:} Ants can recover from temporary dead-ends and discover valid paths in maps where the baseline algorithm would fail. This is particularly valuable in environments with narrow passages or maze-like structures.

\textbf{Reduced Wasted Computation:} Instead of discarding partially-explored paths, ants reuse accumulated knowledge about the search space. Backtracking preserves the pheromone information along successful path segments while only discarding failed branches.

\textbf{Intelligent Exploration:} The decision stack naturally prioritizes recent choices, implementing a depth-first search bias that complements ACO's probabilistic exploration. This helps ants thoroughly explore promising corridors before abandoning them.

\textbf{Adaptive Learning:} The global tabu list enables colony-level learning, where information about structural deadlocks is shared across all ants, preventing redundant failures.

\subsection{Experimental results}
The backtracking mechanism was evaluated on four benchmark maps with varying complexity levels, ranging from simple (Map 1: $5 \times 5$) to complex environments (Map 3: $31 \times 31$). Each configuration was tested with 30 independent runs using standard parameters: 50 ants, 100 iterations, evaporation rate of 0.15, and initial pheromone concentration of $1 \times 10^{-4}$. Table~\ref{tab:backtracking_results} presents the comparative performance of backtracking against the baseline ACO.

\begin{table}[h]
\centering
\caption{Performance comparison: Base ACO vs. Backtracking mechanism}
\label{tab:backtracking_results}
\begin{tabular}{lcccccc}
\hline
\textbf{Map} & \multicolumn{2}{c}{\textbf{Min Length (m)}} & \multicolumn{2}{c}{\textbf{Mean Length (m)}} & \multicolumn{2}{c}{\textbf{Runtime (s)}} \\
\cline{2-7}
 & Base & Backtrack & Base & Backtrack & Base & Backtrack \\
\hline
Map 1 (Simple)   & 5.24 & 5.24 & $5.24 \pm 0.00$ & $5.24 \pm 0.00$ & 0.110 & 0.127 \\
Map 3 (Complex)  & 46.11 & 46.11 & $47.77 \pm 1.08$ & $52.59 \pm 3.73$ & 1.980 & 2.546 \\
Map 7 (Medium)   & 44.21 & 44.21 & $44.59 \pm 0.50$ & $47.23 \pm 2.93$ & 1.388 & 1.541 \\
Map 8 (Medium)   & 22.97 & 22.97 & $22.98 \pm 0.08$ & $23.08 \pm 0.26$ & 0.653 & 0.792 \\
\hline
\end{tabular}
\end{table}

\textbf{Path Quality Analysis:} The results reveal an important trade-off characteristic of the backtracking mechanism. Across all benchmark maps, backtracking preserves optimal minimum path length—matching the baseline's best solutions exactly (5.24m, 46.11m, 44.21m, and 22.97m respectively). This demonstrates that the mechanism does not compromise solution quality when it successfully finds paths. However, the mean path length shows notable degradation, particularly on complex maps: Map 3 experiences a 10.1\% increase in mean length ($47.77 \rightarrow 52.59$m) with variance rising from $\pm1.08$ to $\pm3.73$, while Map 7 shows a 5.9\% increase ($44.59 \rightarrow 47.23$m) with variance expanding from $\pm0.50$ to $\pm2.93$.

\textbf{Variance and Stability:} The substantially increased standard deviation in complex environments (Map 3: 3.5× increase, Map 7: 5.9× increase) indicates reduced result consistency. This behavior stems from the stochastic nature of backtracking decisions: when ants encounter dead-ends, the decision stack may lead them to explore significantly different corridor alternatives, resulting in paths of varying quality. In contrast, simpler maps (Map 1, Map 8) show minimal variance increase, suggesting that backtracking's exploratory overhead is most pronounced in environments with multiple competing path candidates.

\textbf{Computational Overhead:} The backtracking mechanism introduces measurable but acceptable runtime overhead across all maps: +15.5\% (Map 1), +28.6\% (Map 3), +11.0\% (Map 7), and +21.3\% (Map 8). This overhead scales with environment complexity, as more intricate obstacle configurations trigger more frequent backtracking operations. The decision stack management, path restoration, and tabu list lookups collectively contribute to this increased computation time. Notably, the overhead remains sub-linear relative to the map complexity increase, confirming the theoretical $\mathcal{O}(1)$ amortized cost per node visit.

\textbf{Performance Trade-offs and Applicability:} The experimental results indicate that standalone backtracking is not universally beneficial. While it successfully prevents complete search failures by enabling ants to recover from dead-ends—thereby maintaining 100\% success rate across all tested maps—it does so at the cost of reduced path quality consistency and increased runtime. The mechanism is most valuable in scenarios where:

\begin{itemize}
    \item \textit{Success rate is critical}: Environments with extremely narrow passages where baseline ACO frequently fails to find any solution.
    \item \textit{Real-time constraints are relaxed}: Applications that can tolerate 15-30\% longer computation time in exchange for guaranteed path discovery.
    \item \textit{Combined with complementary improvements}: As demonstrated in the Proposed Method configuration (Section 3.7), backtracking shows improved behavior when integrated with cone pheromone initialization and adaptive processing, which help guide the backtracking search toward more promising corridors.
\end{itemize}

\textbf{Synergy with Other Improvements:} Examining the Proposed Method results (which combines all improvements including backtracking) reveals partial recovery from standalone backtracking's weaknesses. On Map 3, the combined approach achieves mean length of $50.30 \pm 1.94$m—still elevated compared to baseline ($47.77 \pm 1.08$m) but substantially better than standalone backtracking ($52.59 \pm 3.73$m). This suggests that cone pheromone initialization and adaptive factors help constrain backtracking's exploratory variance while retaining its deadlock-recovery benefits. The division of labor mechanism, with its soldier/king role specialization, provides diverse decision points that backtracking can exploit more effectively than uniform ant populations.